{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_emotion.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNnKDoPibRm+XLTz8mJ30XB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/tensorflow-projects/blob/main/tweet_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will train a model to recognize tweet emotion using the tweet emotion dataset. "
      ],
      "metadata": {
        "id": "u4QkWPcg9phN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh2xjM0J8uRG"
      },
      "outputs": [],
      "source": [
        "!pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import nlp\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "m2UyD7_l_gXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and visualize dataset"
      ],
      "metadata": {
        "id": "xq5kADUCUK1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tweet emotion dataset from huggingface nlp module\n",
        "dataset = nlp.load_dataset(\"emotion\")"
      ],
      "metadata": {
        "id": "s2CBixsBFZYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "m5e-iqaqRDPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset['train']\n",
        "valid_data = dataset['validation']\n",
        "test_data = dataset['test']"
      ],
      "metadata": {
        "id": "SA4bczcnLVKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain tweets and labels from the data\n",
        "def get_tweet(data):\n",
        "  tweets = [x['text'] for x in data] # obtain tweets\n",
        "  labels = [x['label'] for x in data] # obtain tweets labels\n",
        "  return tweets, labels\n",
        "\n",
        "# obtain training tweets and labels \n",
        "tweets, labels = get_tweet(train_data)\n",
        "valid_tweets, valid_labels = get_tweet(valid_data)\n",
        "\n",
        "# let's see first 10 tweets and corresponding labels\n",
        "for i in range(10):\n",
        "  print(\"({}, {})\".format(tweets[i], labels[i]))"
      ],
      "metadata": {
        "id": "PO5cWTKlRBY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x-1QBUB81FOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process the dataset"
      ],
      "metadata": {
        "id": "AqNDfcCMURxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare tweets"
      ],
      "metadata": {
        "id": "Vinn7ftqfCzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenize the tweets"
      ],
      "metadata": {
        "id": "9ZNfRZFWbuyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# let's tokenize 10,000 most commonly used words\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token=\"UNK\")\n",
        "tokenizer.fit_on_texts(tweets)"
      ],
      "metadata": {
        "id": "KAnS8psxRBcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see the tokinization of a tweet\n",
        "print(\"{} ---> {}\".format(tweets[0], tokenizer.texts_to_sequences([tweets[0]])))"
      ],
      "metadata": {
        "id": "KH1-xRbEb6eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Padding and Truncating sequences"
      ],
      "metadata": {
        "id": "-dnB-6XZbyq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check length of the tweets\n",
        "lengths = [len(twt.split(\" \")) for twt in tweets]\n",
        "plt.hist(lengths, bins=len(set(lengths)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wAKET1tnRBhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 50\n",
        "\n",
        "def get_sequences(tokenizer, tweets):\n",
        "  # tokenize tweets\n",
        "  sequences = tokenizer.texts_to_sequences(tweets)\n",
        "  # obtain padded and truncated sequences\n",
        "  padded_seqs = pad_sequences(sequences, truncating=\"post\", padding=\"post\", maxlen=maxlen)\n",
        "  return padded_seqs"
      ],
      "metadata": {
        "id": "vo-GTlIwZI6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain padded and truncated training sequences\n",
        "padded_train_seq = get_sequences(tokenizer, tweets)\n",
        "padded_valid_seq = get_sequences(tokenizer, valid_tweets)\n",
        "\n",
        "\n",
        "print(\"{}, \\n{}\".format(tweets[0], padded_train_seq[0]))"
      ],
      "metadata": {
        "id": "ty3OxRjEdCu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare labels"
      ],
      "metadata": {
        "id": "uzcU8qX2fILN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = set(labels)\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "-abhXUKDdl5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(labels, bins=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s2tmG6YYkwQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = dict((c, i) for i, c in enumerate(classes))\n",
        "idx_to_class = dict((v, k) for k, v in class_to_idx.items())\n",
        "\n",
        "print(class_to_idx)\n",
        "print(idx_to_class)"
      ],
      "metadata": {
        "id": "Wl2Ix2kyk80f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_to_ids = lambda labels: np.array([class_to_idx.get(x) for x in labels])\n",
        "\n",
        "train_labels = names_to_ids(labels)\n",
        "valid_labels = names_to_ids(valid_labels)\n",
        "print(train_labels[0])"
      ],
      "metadata": {
        "id": "OdCobuLBl9jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a neural network"
      ],
      "metadata": {
        "id": "Vty6NyyIplEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Embedding(10000, 16, input_length=maxlen),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
        "            tf.keras.layers.Dense(6, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ccAnwNghmYvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(padded_train_seq, train_labels, \n",
        "                    validation_data=(padded_valid_seq, valid_labels),\n",
        "                    epochs=20,\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2)])"
      ],
      "metadata": {
        "id": "YOX4sIsRwwOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dX6LSLSCwwTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-C7386k0wwZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LTFrEOYYwwcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}